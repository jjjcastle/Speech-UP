import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision.models import vgg16
from PIL import Image
import mediapipe as mp
from collections import Counter
import os
import torch.nn as nn

def generate_feedback(expression_score, pose_score, pose_log=None, expression_counts=None):
    # í‘œì • í”¼ë“œë°±
    if expression_score >= 90:
        expression_feedback = "ğŸ˜Š í‘œì •ì´ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤!"
    elif expression_score >= 80:
        expression_feedback = "ğŸ™‚ ì „ë°˜ì ìœ¼ë¡œ í‘œì •ì´ ì¢‹ìŠµë‹ˆë‹¤."
    elif expression_score >= 70:
        expression_feedback = "ğŸ˜ ì¡°ê¸ˆ ë” ì›ƒëŠ” í‘œì •ì„ ì—°ìŠµí•´ë³´ì„¸ìš”!"
    elif expression_score >= 60:
        expression_feedback = "ğŸ˜• ì¢‹ì§€ ì•Šì€ í‘œì •ì´ ê°€ë” ë‚˜íƒ€ë‚©ë‹ˆë‹¤."
    else:
        expression_feedback = "ğŸ˜Ÿ í‘œì •ì— ì¡°ê¸ˆ ë” ì‹ ê²½ ì¨ì£¼ì„¸ìš”!"

    # ì‹œì„  í”¼ë“œë°±
    if pose_score >= 90:
        pose_feedback = "ğŸ‘ï¸ ì‹œì„  ì²˜ë¦¬ê°€ ë§¤ìš° ì•ˆì •ì ì…ë‹ˆë‹¤!"
    elif pose_score >= 80:
        pose_feedback = "ğŸ‘€ ì‹œì„ ì´ ì•½ê°„ í”ë“¤ë ¸ì§€ë§Œ ê´œì°®ì€ í¸ì…ë‹ˆë‹¤."
    elif pose_score >= 70:
        pose_feedback = "ğŸ˜¶â€ğŸŒ«ï¸ ì¹´ë©”ë¼ ì‘ì‹œì— ì¡°ê¸ˆë§Œ ì‹ ê²½ ì¨ì£¼ì„¸ìš”!"
    else:
        pose_feedback = "ğŸ˜µ ì‹œì„ ì´ ë§¤ìš° ë¶ˆì•ˆì •í–ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ì‘ì‹œ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤."

    # âœ… ì‹œì„  ì´íƒˆ ìš”ì•½ (ê°€ì¥ ë§ì´ ë‚˜ì˜¨ 1ê°œ ë°©í–¥ë§Œ)
    if pose_log:
        filtered = [gaze for _, _, gaze in pose_log if gaze not in ["Center", "Level"]]
        if pose_score < 85 :
            if filtered:
                most_common_dir, count = Counter(filtered).most_common(1)[0]
                pose_feedback += f"\n  - ì£¼ë¡œ {most_common_dir} ë°©í–¥ì„ ë§ì´ ì³ë‹¤ë´…ë‹ˆë‹¤."

    expression_summary = ""
    if expression_counts:
        frown_count = expression_counts.get("frown", 0)
        neutral_count = expression_counts.get("neutral", 0)
        total_count = sum(expression_counts.values())

        if expression_score < 90:
            if frown_count >= max(5, total_count * 0.3):
                expression_summary += f"  - ë¶ˆì•ˆí•œ í‘œì •ì´ ìì£¼ ë³´ì…ë‹ˆë‹¤. í‘œì • ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤!\n"
            if neutral_count >= max(5, total_count * 0.3):
                expression_summary += f"  - ë¬´í‘œì •ì´ ë§ì´ ë³´ì˜€ìŠµë‹ˆë‹¤. ì›ƒëŠ” í‘œì •ì„ ì—°ìŠµí•´ë³´ì„¸ìš”!.\n"

    # í†µí•© í”¼ë“œë°± ë°˜í™˜
    return f"""{expression_feedback}
{expression_summary if expression_summary else ""}
{pose_feedback}
"""

def run_vision_model(folder_path):
    model_path = "C:/Users/school/Desktop/Speech_To_Text/vgg16_epoch5_sd.pth"
    class_labels = ["neutral", "smile", "frown"]
    fps = 12
    use_cuda = True
    device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")
    model = vgg16(weights=None)
    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, len(class_labels))
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])

    image_files = sorted(
        [f for f in os.listdir(folder_path) if f.lower().endswith((".jpg", ".jpeg", ".png"))],
        key=lambda x: int(x.split("_")[-1].split(".")[0])
    )

    expression_score = 100
    expression_counts = {label: 0 for label in class_labels}
    prev_expr = None
    confirmed_expr = None
    expression_streak = 0
    expression_log = []

    pose_score = 100
    pose_log = []
    prev_pose = None
    pose_count = 0

    prev_pitch = None
    pitch_count = 0

    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)

    def apply_penalty(expr, streak):
        if expr == "frown" and streak >= 4:
            return 5
        elif expr == "neutral" and streak >= 4:
            return 1
        return 0

    def apply_bonus(expr, streak):
        if expr == "smile" and streak >= 12:
            return 1
        return 0

    for i, fname in enumerate(image_files):
        img_path = os.path.join(folder_path, fname)
        image = cv2.imread(img_path)
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {img_path}")
            continue

        h, w, _ = image.shape
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(image_rgb)
        input_tensor = transform(pil_img).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(input_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]
            max_prob = np.max(probs)
            max_idx = np.argmax(probs)

        if max_prob >= 0.5:
            expr = class_labels[max_idx]
            #print(f"[í‘œì • ì˜ˆì¸¡] Frame {i}: í™•ë¥ ={max_prob:.2f}, ì˜ˆì¸¡ í‘œì •={expr}")
            expression_counts[expr] += 1

            expression_streak = expression_streak + 1 if expr == prev_expr else 1
            prev_expr = expr
        else:
            continue

        # ê°ì  êµ¬ê°„
        if expression_streak >= 6 and expression_streak % (fps * 2) == 0:
            penalty = apply_penalty(expr, expression_streak)
            if penalty > 0:
                expression_score -= penalty
                print(f"[í‘œì • -] Frame {i}: {expr} ì§€ì†ìœ¼ë¡œ -{penalty}ì  â†’ ì ìˆ˜: {expression_score}")
                expression_log.append((i, expr))

        # ë³´ë„ˆìŠ¤ êµ¬ê°„
        if expression_streak >= 24 and expression_streak % (fps * 2) == 0:
            bonus = apply_bonus(expr, expression_streak)
            if bonus > 0:
                expression_score += bonus
                print(f"[í‘œì • +] Frame {i}: {expr} ì§€ì†ìœ¼ë¡œ +{bonus}ì  â†’ ì ìˆ˜: {expression_score}")
                expression_log.append((i, expr))

        # í‘œì • ë³€í™”ì— ë”°ë¥¸ ì ìˆ˜ ë³€í™”
        if expression_streak >= 6 and expr != confirmed_expr:
            before_expr = confirmed_expr
            confirmed_expr = expr
            expression_counts[expr] += 1

            if expr == "smile":
                expression_score += 5
            elif expr == "frown":
                expression_score -= 5
            elif expr == "neutral":
                expression_score -= 1

            print(f"[í‘œì • ë³€í™”] Frame {i}: í‘œì • {before_expr} â†’ {expr}, ì ìˆ˜: {expression_score}")
            expression_log.append((i, expr))  # âœ… í™•ì • ì ìˆ˜ ë³€ê²½ë„ ë¡œê·¸




        # ì‹œì„  ë¶„ì„
        gaze_direction = "Center"
        pitch_dir = "Level"
        result = face_mesh.process(image_rgb)

        if result.multi_face_landmarks:
            landmarks = result.multi_face_landmarks[0].landmark
            face_points = {"nose_tip": 1, "chin": 152, "left_eye_corner": 263,
                           "right_eye_corner": 33, "left_mouth_corner": 287, "right_mouth_corner": 57}
            model_points = np.array([
                (0.0, 0.0, 0.0), (0.0, -63.6, -12.5), (-43.3, 32.7, -26.0),
                (43.3, 32.7, -26.0), (-28.9, -28.9, -24.1), (28.9, -28.9, -24.1)
            ], dtype=np.float64)

            image_points = np.array([
                (landmarks[face_points["nose_tip"]].x * w, landmarks[face_points["nose_tip"]].y * h),
                (landmarks[face_points["chin"]].x * w, landmarks[face_points["chin"]].y * h),
                (landmarks[face_points["left_eye_corner"]].x * w, landmarks[face_points["left_eye_corner"]].y * h),
                (landmarks[face_points["right_eye_corner"]].x * w, landmarks[face_points["right_eye_corner"]].y * h),
                (landmarks[face_points["left_mouth_corner"]].x * w, landmarks[face_points["left_mouth_corner"]].y * h),
                (landmarks[face_points["right_mouth_corner"]].x * w, landmarks[face_points["right_mouth_corner"]].y * h)
            ], dtype=np.float64)

            cx = landmarks[face_points["nose_tip"]].x * w
            cy = landmarks[face_points["nose_tip"]].y * h
            camera_matrix = np.array([[w, 0, cx], [0, w, cy], [0, 0, 1]], dtype=np.float64)
            dist_coeffs = np.zeros((4, 1))
            _, rot_vec, _ = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)
            rmat, _ = cv2.Rodrigues(rot_vec)
            angles, _, _, _, _, _ = cv2.RQDecomp3x3(rmat)
            yaw, pitch = angles[1], angles[0]

            if yaw > 20:
                gaze_direction = "Right"
            elif yaw < -20:
                gaze_direction = "Left"
            else:
                gaze_direction = "Center"

            if pitch > 15:
                pitch_dir = "Down"
            elif pitch < -15:
                pitch_dir = "Up"
            else:
                pitch_dir = "Level"
            #print(f"[ì‹œì„  ì˜ˆì¸¡] Frame {i}: Gaze â†’ {gaze_direction}, Pitch â†’ {pitch_dir}")

        frame_index = int(os.path.splitext(fname)[0].split("_")[-1])
        timestamp = frame_index / fps

        if gaze_direction == prev_pose:
            pose_count += 1
        else:
            pose_count = 1
            prev_pose = gaze_direction

        # Pitch ê¸°ë°˜ ê°ì  (Down ë˜ëŠ” Up)
        if pitch_dir == prev_pitch:
            pitch_count += 1
        else:
            pitch_count = 1
            prev_pitch = pitch_dir

            # Yaw ê¸°ë°˜ ê°ì 
        if gaze_direction in ["Left", "Right"]:
            if pose_count >= fps and pose_count % fps == 0:
                pose_score -= 3
                print(f"[ì‹œì„  -] {gaze_direction} ë°©í–¥ ì§€ì† â†’ -3ì  â†’ ì ìˆ˜: {pose_score}")
                pose_log.append((i, timestamp, gaze_direction))

        if pitch_dir in ["Up", "Down"]:
            if pitch_count >= fps and pitch_count % fps == 0:
                pose_score -= 2
                print(f"[ì‹œì„  -] {pitch_dir} ë°©í–¥ ê¸°ìš¸ì–´ì§ ì§€ì† â†’ -2ì  â†’ ì ìˆ˜: {pose_score}")
                pose_log.append((i, timestamp, pitch_dir))


    log_path = os.path.join(folder_path, "image_expression_log.txt")
    with open(log_path, "w", encoding="utf-8") as f:
        for frame_idx, expr in expression_log:
            time_sec = frame_idx / fps
            f.write(f"{frame_idx}í”„ë ˆì„ ({time_sec:.2f}ì´ˆ): {expr}\n")

    gaze_log_path = os.path.join(folder_path, "image_gaze_log.txt")
    with open(gaze_log_path, "w", encoding="utf-8") as f:
        for frame_index, timestamp, gaze in pose_log:
            f.write(f"{frame_index}í”„ë ˆì„ ({timestamp:.2f}ì´ˆ): {gaze}\n")

    feedback = generate_feedback(expression_score, pose_score, pose_log, expression_counts)
    with open(os.path.join(folder_path, "image_final_feedback.txt"), "w", encoding="utf-8") as f:
        f.write(feedback)

    print(f"í‘œì • ë¡œê·¸ ì €ì¥ë¨: {log_path}")
    print(f"ì‹œì„  ë¡œê·¸ ì €ì¥ë¨: {gaze_log_path}")
    print(feedback)
    print(f'''ğŸ“Š ìµœì¢… ì ìˆ˜ ìš”ì•½ :
        - í‘œì • ì ìˆ˜: {expression_score}
        - ì‹œì„  ì ìˆ˜: {pose_score}''')
    return feedback

# run_vision_model(folder_path='C:/Users/school/Desktop/Speech_To_Text/captured_frames/32')