
# ğŸ§  LLaMA3 ê¸°ë°˜ ë¬¸ì¥ êµì • ëª¨ë“ˆ (Corrector)

ì´ í”„ë¡œì íŠ¸ëŠ” LoRA ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ LLaMA3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ëª»ëœ ì˜ì–´ ë¬¸ì¥ì„ êµì •í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.  
Whisper ë“±ìœ¼ë¡œë¶€í„° ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´, ì˜¬ë°”ë¥¸ ë¬¸ì¥ìœ¼ë¡œ êµì •í•´ì£¼ëŠ” ê¸°ëŠ¥ì„ `corrector.py`ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“ íŒŒì¼ êµ¬ì„±

| íŒŒì¼ëª… | ì„¤ëª… |
|--------|------|
| `corrector.py` | ğŸ‘‰ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ë¬¸ì¥ì„ êµì •í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. (`correct_sentence(text: str)` í•¨ìˆ˜) |
| `gpt_corrected.jsonl` | ğŸ‘‰ íŒŒì¸íŠœë‹ì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ (í•™ìŠµìš© ì°¸ê³ ) |
| `llama3_lora_adapter.zip` | âœ… [í•„ìˆ˜] íŒŒì¸íŠœë‹ëœ LoRA ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„° |
| `llama3_lora_ckpt.zip` | (ì„ íƒ) ì „ì²´ í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ (ì¬í•™ìŠµ ì‹œ ì‚¬ìš© ê°€ëŠ¥) |
| `lora_output.zip` | (ì„ íƒ) tokenizer ì €ì¥ í´ë” (ì¶”ë¡ ì—ëŠ” ì—†ì–´ë„ ë¨) |
| `LLM_finetuning.ipynb` | íŒŒì¸íŠœë‹ ì „ì²´ ì½”ë“œ (í•™ìŠµ êµ¬ì¡° ì°¸ê³ ìš©) |

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

```bash
pip install torch transformers peft accelerate
```

ë˜ëŠ” Conda í™˜ê²½ ì‚¬ìš© ì‹œ:

```bash
conda create -n llama_env python=3.10
conda activate llama_env
pip install torch transformers peft accelerate
```

---

## ğŸ“¦ ì••ì¶• í•´ì œ (ë°˜ë“œì‹œ ë¨¼ì € ìˆ˜í–‰)

```bash
unzip llama3_lora_adapter.zip
```

> ì´ ë””ë ‰í† ë¦¬ê°€ `corrector.py`ì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

---

## âœ… ì‚¬ìš©ë²• ì˜ˆì‹œ

### 1. íŒŒì´ì¬ ì½”ë“œì—ì„œ ì‚¬ìš©

```python
from corrector import correct_sentence

text = "this are the example sentense i want to see if it correct."
result = correct_sentence(text)
print(result)
```

### 2. Flask ë°±ì—”ë“œ ì—°ë™ ì˜ˆì‹œ

```python
from flask import Flask, request, jsonify
from corrector import correct_sentence

app = Flask(__name__)

@app.route("/correct", methods=["POST"])
def correct_api():
    input_text = request.json.get("text", "")
    output = correct_sentence(input_text)
    return jsonify({"corrected": output})

if __name__ == "__main__":
    app.run()
```

---

## â— ì£¼ì˜ ì‚¬í•­

- ëª¨ë¸ì€ `"meta-llama/Llama-3.1-8B-Instruct"` ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
- GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ ë¹ ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤. (ìµœì†Œ 16GB VRAM ê¶Œì¥)
- `llama3_lora_adapter/` í´ë”ëŠ” ë°˜ë“œì‹œ `corrector.py`ì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
- `corrector.py` ë‚´ë¶€ì—ì„œ ëª¨ë¸ ë¡œë”©ì€ ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

---

## ğŸ“¬ ë¬¸ì˜

ëª¨ë“ˆ ê´€ë ¨ ë¬¸ì˜ëŠ” [ì¤€ì„œ]ì—ê²Œ í•´ ì£¼ì„¸ìš”.
